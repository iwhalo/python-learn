# ä¸€æ–‡å¸¦ä½ äº†è§£å¹¶å‘ç¼–ç¨‹ï¼šçº¿ç¨‹ã€è¿›ç¨‹ä¸åç¨‹

åœ¨ Python ä¸­ï¼Œ **å¹¶å‘ç¼–ç¨‹** è®©ç¨‹åºèƒ½å¤ŸåŒæ—¶æ‰§è¡Œå¤šä¸ªä»»åŠ¡ï¼Œæ˜¾è‘—æé«˜æ•ˆç‡ã€‚ä¸»è¦çš„å¹¶å‘æ–¹æ¡ˆåŒ…æ‹¬ã€å¤šçº¿ç¨‹ã€‘ã€ã€å¤šè¿›ç¨‹ã€‘å’Œã€åç¨‹ã€‘ã€‚æœ¬æ–‡å°†æ·±å…¥æµ…å‡ºåœ°ä»‹ç»è¿™äº›æ¦‚å¿µã€é€‚ç”¨åœºæ™¯ï¼Œå¹¶æä¾›ä¼˜åŒ–åçš„ä»£ç ç¤ºä¾‹ï¼Œå¸®åŠ©ä½ è½»æ¾æŒæ¡å¹¶å‘ç¼–ç¨‹ã€‚

## 1\. è¿›ç¨‹ä¸çº¿ç¨‹åŸºç¡€

### 1.1 è¿›ç¨‹ä¸çº¿ç¨‹çš„å…³ç³»

  *  **è¿›ç¨‹** ï¼šæ“ä½œç³»ç»Ÿèµ„æºåˆ†é…çš„æœ€å°å•ä½ï¼Œæ‹¥æœ‰ç‹¬ç«‹çš„å†…å­˜ç©ºé—´ï¼Œå„è¿›ç¨‹ç›¸äº’éš”ç¦»ã€‚
  *  **çº¿ç¨‹** ï¼šCPU è°ƒåº¦çš„æœ€å°æ‰§è¡Œå•ä½ï¼ŒåŒä¸€è¿›ç¨‹å†…çš„å¤šä¸ªçº¿ç¨‹å…±äº«è¯¥è¿›ç¨‹çš„èµ„æºã€‚



ğŸŒŸ **å½¢è±¡æ¯”å–»** ï¼šè¿›ç¨‹å¥½æ¯”ä¸€ä¸ªå·¥å‚ï¼Œçº¿ç¨‹åˆ™æ˜¯å·¥å‚é‡Œçš„å·¥äººã€‚å¤šçº¿ç¨‹å°±åƒå¤šä¸ªå·¥äººåœ¨åŒä¸€å·¥å‚åä½œï¼Œå¤šè¿›ç¨‹åˆ™æ˜¯å¼€è®¾å¤šä¸ªå·¥å‚å¹¶è¡Œå·¥ä½œã€‚

### 1.2 GILï¼ˆå…¨å±€è§£é‡Šå™¨é”ï¼‰

CPython è§£é‡Šå™¨çš„ GIL æœºåˆ¶ä½¿å¾—å³ä½¿å¯ç”¨å¤šçº¿ç¨‹ï¼ŒåŒä¸€æ—¶åˆ»ä¹Ÿåªæœ‰ä¸€ä¸ªçº¿ç¨‹åœ¨æ‰§è¡Œ Python å­—èŠ‚ç ã€‚å› æ­¤ï¼š

  *  **è®¡ç®—å¯†é›†å‹ä»»åŠ¡** ï¼ˆå¤§é‡æ•°å­¦è¿ç®—ã€æ•°æ®å¤„ç†ï¼‰æ›´é€‚åˆä½¿ç”¨å¤šè¿›ç¨‹ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ CPUã€‚
  *  **IO å¯†é›†å‹ä»»åŠ¡** ï¼ˆæ–‡ä»¶è¯»å†™ã€ç½‘ç»œè¯·æ±‚ï¼‰é€‚åˆä½¿ç”¨å¤šçº¿ç¨‹æˆ–åç¨‹ï¼Œå› ä¸ºåœ¨ç­‰å¾… IO æ—¶å¯ä»¥åˆ‡æ¢åˆ°å…¶ä»–ä»»åŠ¡ã€‚



## 2\. å¤šçº¿ç¨‹ç¼–ç¨‹

å¤šçº¿ç¨‹ç‰¹åˆ«é€‚åˆå¤„ç† IO å¯†é›†å‹ä»»åŠ¡ï¼Œä¾‹å¦‚å¹¶è¡Œä¸‹è½½å¤šä¸ªæ–‡ä»¶ã€‚ä¸‹é¢å±•ç¤ºä¸€ä¸ªä¼˜åŒ–çš„å¤šçº¿ç¨‹ä¸‹è½½ç¤ºä¾‹ï¼š

```python
import threading
import requests
import time
import logging
from concurrent.futures import ThreadPoolExecutor

from typing import List,Tuple

from Crypto.SelfTest.Cipher.test_CFB import file_name
from django.db.models.expressions import result

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)s - %(levelName)s - %(message)s'
)
logger=logging.getLogger(__name__)

class DownloadManager:
    """æ–‡ä»¶ä¸‹è½½ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œä¸‹è½½"""
    def __init__(self,max_workers:int=5):
        """
        åˆå§‹åŒ–ä¸‹è½½ç®¡ç†å™¨
        å‚æ•°ï¼š
            max_workersï¼šæœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers=max_workers

        # ä½¿ç”¨çº¿ç¨‹æ± ç®¡ç†çº¿ç¨‹èµ„æº
        self.executor=ThreadPoolExecutor(max_workers=self.max_workers,thread_name_prefix="Downloader")
    def download_file(self,file_name:str,url:str,timeout:int=30)->bool:
        """
        ä¸‹è½½å•ä¸ªæ–‡ä»¶çš„æ–¹æ³•ï¼š
        å‚æ•°ï¼š
            file_nameï¼šä¿å­˜çš„æ–‡ä»¶åå­—
            urlï¼šä¸‹è½½æ–‡ä»¶çš„urlåœ°å€
            timeoutï¼šè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        è¿”å›ï¼š
            boolï¼šä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½ {file_name}")
            start_time=time.time()

            # æ·»åŠ è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è¡Œä¸º
            headers={
                "User-Agent":"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
            }

            # ä½¿ç”¨è¶…æ—¶å’Œæµå¼ä¸‹è½½ å¤„ç†å¤§æ–‡ä»¶
            with requests.get(url=url,headers=headers,stream=True,timeout=timeout) as response:
                # ç¡®ä¿è¯·æ±‚æˆåŠŸ
                response.raise_for_status()
                # è·å–æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœæœåŠ¡å™¨æä¾›ï¼‰
                total_size=int(response.headers.get('content-length',0))

                with open(file_name,'wb') as f:
                    # åˆ†å¿«ä¸‹è½½ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶åˆ°å†…å­˜
                    chunk_size=8192 # 8kb
                    downloaded=0
                    for chunk in response.iter_content(chunk_size=chunk_size):
                        # è¿‡æ»¤ä¿æŒé“¾æ¥æ´»è·ƒå¾—ç©ºå—
                        if chunk:
                            f.write(chunk)
                            downloaded+=len(chunk)

                            # è®°å½•ä¸‹è½½è¿›åº¦
                            if total_size>0:
                                progress=(downloaded/total_size)*100
                            if downloaded%(5*chunk_size)==0: # æ¯ä¸‹è½½çº¦40KBæ›´æ–°ä¸€æ¬¡è¿›åº¦
                                logger.debug(f"{file_name} - ä¸‹è½½è¿›åº¦ï¼š{progress:.1f}%")
                elapsed=time.time()-start_time
                logger.info(f"{file_name} ä¸‹è½½å®Œæˆ - è€—æ—¶ï¼š{elapsed:.2f}ç§’")
                return True
        except requests.exceptions.RequestException as e:
            logger.error(f"ä¸‹è½½ {file_name} å¤±è´¥ï¼š{str(e)}")
            return False
        except IOError as e:
            logger.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯ {file_name}ï¼š{str(e)}")
            return False
        except Exception as e:
            logger.error(f"ä¸‹è½½ {file_name} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}")
            return False

    def download_multiple(self,url_list:List[Tuple[str,str]])->List[bool]:
        """
        å¹¶è¡Œä¸‹è½½å¤šä¸ªæ–‡ä»¶
        å‚æ•°ï¼š
            url_list:åŒ…å«ï¼ˆæ–‡ä»¶åï¼ŒURLï¼‰å…ƒç»„çš„åˆ—è¡¨
        è¿”å›ï¼š
            List[bool]:æ¯ä¸ªä¸‹è½½ä»»åŠ¡çš„æˆåŠŸçŠ¶æ€åˆ—è¡¨
        """

        logger.info(f"å¼€å§‹ä¸‹è½½ {len(url_list)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªçº¿ç¨‹")

        # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
        futures=[
            self.executor.submit(self.download_file,file_name,url) for file_name,url in url_list
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        results=[]
        for future in futures:
            try:
                result=future.result()
                results.append(result)
            except Exception as e:
                logger.error(f"æ‰§è¡Œä»»åŠ¡æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{str(e)}")
                results.append(False)

        # ç»Ÿè®¡æˆåŠŸç‡
        success_count=sum(results)
        logger.info(f"æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆï¼æˆåŠŸï¼š{success_count}/{len(url_list)}")

        return results

    def shutdown(self):
        """å…³é—­çº¿ç¨‹æ± ï¼Œé‡Šæ”¾èµ„æº"""
        self.executor.shutdown(wait=True)
        logger.info("ä¸‹è½½ç®¡ç†å™¨å·²å…³é—­")


if __name__ == '__main__':
    # æµ‹è¯•URLåˆ—è¡¨
    url_list=[
        ("video1.mp4","https://example.com/video1"),
        ("video2.mp4","https://example.com/video2"),
        ("video3.mp4","https://example.com/video3")
    ]

    # åˆ›å»ºä¸‹è½½ç®¡ç†å™¨ï¼Œå¹¶å¼€å§‹ä¸‹è½½
    downloader=DownloadManager(max_workers=3)
    try:
        results=downloader.download_multiple(url_list)
    finally:
    #     ç¡®ä¿èµ„æºè¢«æ­£ç¡®é‡Šæ”¾
        downloader.shutdown()




```
    

### çº¿ç¨‹ç®¡ç†çš„å…³é”®ç‚¹

  *  **çº¿ç¨‹æ± ** ï¼šä½¿ç”¨ `ThreadPoolExecutor` ç®¡ç†çº¿ç¨‹èµ„æºï¼Œé¿å…é¢‘ç¹åˆ›å»ºå’Œé”€æ¯çº¿ç¨‹çš„å¼€é”€ã€‚
  *  **å¼‚å¸¸å¤„ç†** ï¼šå¯¹æ¯ä¸ªä¸‹è½½ä»»åŠ¡è¿›è¡Œå®Œå–„çš„å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿å•ä¸ªä»»åŠ¡å¤±è´¥ä¸ä¼šå½±å“æ•´ä½“ç¨‹åºã€‚
  *  **èµ„æºç®¡ç†** ï¼šé€šè¿‡ `with` è¯­å¥å’Œ `shutdown()` æ–¹æ³•ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾ã€‚
  *  **æ—¥å¿—è®°å½•** ï¼šä½¿ç”¨ `logging` æ¨¡å—æ›¿ä»£ç®€å•çš„ `print`ï¼Œæ–¹ä¾¿è°ƒè¯•å’Œé—®é¢˜è¿½è¸ªã€‚



### çº¿ç¨‹é”ä¿è¯æ•°æ®å®‰å…¨

å½“å¤šä¸ªçº¿ç¨‹æ“ä½œå…±äº«æ•°æ®æ—¶ï¼Œéœ€è¦ä½¿ç”¨é”æœºåˆ¶é˜²æ­¢æ•°æ®ç«äº‰é—®é¢˜ï¼š
    
```python
import threading
import logging
import time


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(threadName)s - %(levelname)s - %(message)s'
)

logger=logging.getLogger(__name__)

class ThreadSafeCounter:
    """çº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨å®ç°"""
    def __init__(self,initial_value:int=0):
        """
        åˆå§‹åŒ–è®¡æ•°å™¨
        å‚æ•°ï¼š
            initial_valueï¼šè®¡æ•°å™¨åˆå§‹å€¼
        """
        self._value=initial_value
        self._lock=threading.RLock()   # å¯é‡å…¥é”ï¼Œæ”¯æŒåŒä¸€çº¿ç¨‹å¤šæ¬¡è·å–

    def increment(self,amount:int=1)->int:
        """
        å¢åŠ è®¡æ•°å™¨å€¼  å¹¶è¿”å›æ–°å€¼
        å‚æ•°ï¼š
            amountï¼šå¢åŠ çš„æ•°é‡
        è¿”å›ï¼š
            intï¼šå¢åŠ åçš„è®¡æ•°å™¨å€¼
        """
        with self._lock:
            self._value+=amount
            current=self._value
        return current

    def decrement(self,amount:int=1)->int:
        """
        å‡å°‘è®¡æ•°å™¨å€¼ å¹¶è¿”å›æ–°å€¼
        å‚æ•°ï¼š
            amountï¼šå‡å°‘çš„æ•°é‡
        è¿”å›ï¼š
            intï¼šå‡å°‘åçš„è®¡æ•°å™¨å€¼
        """
        with self._lock:
            self._value-=amount
            current=self._value
        return current

    @property
    def value(self)->int:
        """
        è·å–å½“å‰è®¡æ•°å™¨å€¼
        è¿”å›ï¼š
            intï¼šå½“å‰è®¡æ•°å™¨å€¼
        """
        with self._lock:
            return self._value

def worker(counter:ThreadSafeCounter,iterations:int,worker_id:int):
    """
    å·¥ä½œçº¿ç¨‹å‡½æ•°ï¼Œæ‰§è¡ŒæŒ‡å®šæ¬¡æ•°è®¡æ•°å™¨å¢åŠ æ“ä½œ
    å‚æ•°ï¼š
    counterï¼šçº¿ç¨‹å®‰å…¨çš„è®¡æ•°å™¨å¯¹è±¡
    iterationsï¼šè¿­ä»£æ¬¡æ•°
    worker_idï¼šå·¥ä½œçº¿ç¨‹ID
    """
    logger.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} å¼€å§‹æ‰§è¡Œ")

    for i in range(iterations):
        # æ¨¡æ‹Ÿä¸€äº›éšæœºå·¥ä½œé‡
        if i%10000==0:
            logger.debug(f"å·¥ä½œçº¿ç¨‹ {worker_id} å·²å®Œæˆ {i} æ¬¡æ“ä½œ")
        # å¢åŠ è®¡æ•°å™¨
        counter.increment()

    logger.info(f"å·¥ä½œçº¿ç¨‹ {worker_id} å®Œæˆï¼Œå…±æ‰§è¡Œ {iterations} æ­¤æ“ä½œ")

def run_threaded_counter_test(num_threads:int=5,iterations_per_thread:int=100000):
    """
    è¿è¡Œå¤šçº¿ç¨‹è®¡æ•°å™¨æµ‹è¯•
    å‚æ•°ï¼š
        num_threadsï¼šçº¿ç¨‹æ•°é‡
        iterations_per_threadï¼šæ¯ä¸ªçº¿ç¨‹æ‰§è¡Œçš„è¿­ä»£æ¬¡æ•°
    """
    # åˆ›å»ºçº¿ç¨‹å®‰å…¨è®¡æ•°å™¨
    counter=ThreadSafeCounter()

    # åˆ›å»ºçº¿ç¨‹åˆ—è¡¨
    threads:List[threading.Thread]=[]
    logger.info(f"å¼€å§‹æµ‹è¯•ï¼š{num_threads} ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹ {iterations_per_thread} æ­¤æ“ä½œ")
    start_time=time.time()

    # åˆ›å»ºå¹¶å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
    for i in range(num_threads):
        t=threading.Thread(
            target=worker,
            args=(counter,iterations_per_thread,i),
            name=f"Worker-{i}"
        )
        threads.append(t)
        t.start()

    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in threads:
        t.join()

    elapsed=time.time()-start_time


    # éªŒè¯ç»“æœ
    expected=num_threads*iterations_per_thread
    actual=counter.value

    logger.info(f"æµ‹è¯•å®Œæˆï¼è€—æ—¶ï¼š{elapsed:.2f} ç§’")
    logger.info(f"è®¡æ•°å™¨æœ€ç»ˆå€¼ï¼š{actual}")
    logger.info(f"æœŸæœ›å€¼ï¼š{expected}")
    logger.info(f"ç»“æœ{'æ­£ç¡®' if actual==expected else 'ä¸æ­£ç¡®'}")

if __name__ == '__main__':
    run_threaded_counter_test(num_threads=5,iterations_per_thread=100000)

``` 
    

## 3\. å¤šè¿›ç¨‹ç¼–ç¨‹

å¤šè¿›ç¨‹èƒ½å¤Ÿç»•è¿‡ GIL é™åˆ¶ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ CPUï¼Œç‰¹åˆ«é€‚åˆè®¡ç®—å¯†é›†å‹ä»»åŠ¡ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„å¤šè¿›ç¨‹è®¡ç®—ç¤ºä¾‹ï¼š
    
```python
import multiprocessing as mp
import time
import logging
import os
import math
from platform import processor
from typing import List, Tuple, Dict, Any

# é…ç½®æ—¥å¿—
"""
å¸¸ç”¨çš„æ—¥å¿—è®°å½•å±æ€§åŒ…æ‹¬ï¼š

%(name)s - è®°å½•å™¨åç§°

%(levelname)s - æ—¥å¿—çº§åˆ«åç§°ï¼ˆå°å†™ï¼‰

%(levelno)s - æ—¥å¿—çº§åˆ«æ•°å­—

%(pathname)s - æºæ–‡ä»¶è·¯å¾„

%(filename)s - æ–‡ä»¶å

%(module)s - æ¨¡å—å

%(funcName)s - å‡½æ•°å

%(lineno)d - è¡Œå·

%(message)s - æ—¥å¿—æ¶ˆæ¯
"""
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger(__name__)

# ä½¿ç”¨æ›´å¥å£®çš„æ—¥å¿—é…ç½®
def get_logger(name=None):
    logger=logging.getLogger(name)
    if not logger.handlers:
        handler=logging.StreamHandler()
        formatter=logging.Formatter(
            '%(asctime)s - %(processName)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger
# ä½¿ç”¨æ–¹å¼
logger=get_logger(__name__)

class ParallelProcessor:
    """å¹¶è¡Œä»»åŠ¡å¤„ç†å™¨ï¼ŒåŸºäºå¤šè¿›ç¨‹æ‰§è¡Œ"""

    def __init__(self, num_processes: int = None):
        """
        åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨
        å‚æ•°ï¼š
            num_processesï¼šè¿›ç¨‹æ•°é‡ï¼Œé»˜è®¤ä¸ºCPUçš„æ ¸å¿ƒæ•°
        """
        # è‹¥æœªæŒ‡å®šè¿›ç¨‹æ•°ï¼Œåˆ™ä½¿ç”¨CPUæ ¸å¿ƒæ•°
        self.num_processes = num_processes or mp.cpu_count()
        logger.info(f"åˆå§‹åŒ–å¹¶è¡Œå¤„ç†å™¨ï¼Œä½¿ç”¨ {self.num_processes} ä¸ªè¿›ç¨‹")

    def _worker_calc_partial_sum(self, task_id: int, start: int, end: int, result_queue: mp.Queue) -> None:
        """
        å·¥ä½œè¿›ç¨‹å‡½æ•°ï¼šè®¡ç®—éƒ¨åˆ†å’Œ
        å‚æ•°ï¼š
            task_idï¼šä»»åŠ¡id
            startï¼šèµ·å§‹å€¼ï¼ˆåŒ…å«ï¼‰
            endï¼šç»“æŸå€¼ï¼ˆåŒ…å«ï¼‰
            result_queueï¼šç»“æœé˜Ÿåˆ—
        """
        try:
            process_id = os.getpid()
            logger.info(f"ä»»åŠ¡ {task_id} å¼€å§‹äºè¿›ç¨‹ {process_id} ï¼Œè®¡ç®—èŒƒå›´ï¼š[{start},{end}]")
            # ç”¨æ•°å­¦å…¬å¼è®¡ç®—åŒºé—´å’Œï¼Œæ¯”å¾ªç¯æ›´é«˜æ•ˆ
            # sum(range(start,end)) = (end - 1 + start) * (end - start) / 2
            result = (end - 1 + start) * (end - start) // 2
            # ä¹Ÿå¯ä»¥ç”¨å†…ç½®sumå‡½æ•°ï¼Œä½†åœ¨ç‰¹å¤§èŒƒå›´æ—¶å¯èƒ½æ•ˆç‡è¾ƒä½
            # result = sum(range(start,end))

            # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
            result_queue.put((task_id, result))
            logger.info(f"ä»»åŠ¡ {task_id} å®Œæˆï¼Œç»“æœï¼š{result}")
        except Exception as e:
            logger.error(f"ä»»åŠ¡ {task_id} å‡ºé”™ï¼š{str(e)}")
            # æ”¾å…¥é”™è¯¯ç»“æœ
            result_queue.put((task_id, None))

    def calculate_sum(self, start: int, end: int) -> int:
        """
        å¹¶è¡Œè®¡ç®—ä»startåˆ°end-1çš„æ•´æ•°å’Œ
        å‚æ•°ï¼š
            startï¼šèµ·å§‹å€¼ï¼ˆåŒ…å«ï¼‰
            endï¼šç»“æŸå€¼ï¼ˆåŒ…å«ï¼‰
        è¿”å›ï¼š
            intï¼šè®¡ç®—ç»“æœ
        """
        if end <= start:
            return 0
        # åˆ›å»ºé€šä¿¡é˜Ÿåˆ—
        result_queue = mp.Queue()

        # è®¡ç®—æ¯ä¸ªè¿›ç¨‹çš„å·¥ä½œé‡
        total_range = end - start
        chunk_size = math.ceil(total_range / self.num_processes)

        # åˆ›å»ºè¿›ç¨‹åˆ—è¡¨
        processes = []
        logger.info(f"å¼€å§‹è®¡ç®—ä» {start} åˆ° {end} çš„å’Œï¼Œåˆ†ä¸º {self.num_processes} ä¸ªå­ä»»åŠ¡")
        start_time = time.time()

        # åˆ›å»ºå¹¶è‡ªå¯åŠ¨æ‰€æœ‰è¿›ç¨‹
        for i in range(self.num_processes):
            task_start = start + i * chunk_size
            task_end = min(task_start + chunk_size, end)
            # è‹¥å·²è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡åˆ›å»ºè¿›ç¨‹
            if task_start >= end:
                continue

            p = mp.Process(
                target=self._worker_calc_partial_sum,
                args=(i, task_start, task_end, result_queue),
                name=f"Calculator-{i}"
            )
            processes.append(p)
            p.start()

        # æ”¶é›†ç»“æœ
        results = {}
        for _ in range(len(processes)):
            task_id, value = result_queue.get()
            if value is not None:
                results[task_id] = value

        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹ç»“æŸ
        for p in processes:
            p.join()

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½æˆåŠŸå®Œæˆ
        if len(results) != len(processes):
            logger.warning(f"éƒ¨åˆ†ä»»åŠ¡å¤±è´¥ï¼åªæ”¶åˆ° {len(results)}/{len(processes)} ä¸ªç»“æœ")

        # è®¡ç®—æ€»å’Œ
        total_sum = sum(results.values())
        elapsed = time.time() - start_time
        logger.info(f"è®¡ç®—å®Œæˆï¼è€—æ—¶ï¼š{elapsed:.2f} ç§’")
        logger.info(f"ç»“æœï¼š{total_sum}")

        return total_sum


# éªŒè¯å‡½æ•°ï¼Œç¡®è®¤å¹¶è¡Œè®¡ç®—ç»“æœæ­£ç¡®æ€§
def verify_sum(start: int, end: int, result: int) -> bool:
    """éªŒè¯è®¡ç®—ç»“æœæ˜¯å¦æ­£ç¡®"""
    # ä½¿ç”¨æ•°å­¦å…¬å¼è®¡ç®—æ­£ç¡®ç­”æ¡ˆ
    expected = (end - 1 + start) * (end - start) // 2
    logger.info(f"éªŒè¯ç»“æœ - è®¡ç®—å€¼ï¼š{result}ï¼ŒæœŸæœ›å€¼ï¼š{expected}")
    return result == expected


if __name__ == '__main__':
    # è®¡ç®—ä»0åˆ°1äº¿çš„å’Œ
    START = 0
    END = 100_000_000

    processor = ParallelProcessor()
    result = processor.calculate_sum(START, END)
    # éªŒè¯ç»“æœ
    is_correct = verify_sum(START, END, result)
    print(f"\nè®¡ç®—ç»“æœï¼š{result}")
    print(f"\nç»“æœéªŒè¯ï¼š{'âœ“ æ­£ç¡®' if is_correct else 'âœ— é”™è¯¯'}")

```   
    

### è¿›ç¨‹é—´æ•°æ®å…±äº«

ä¸åŒè¿›ç¨‹é—´å†…å­˜ç©ºé—´éš”ç¦»ï¼Œæ•°æ®ä¸èƒ½ç›´æ¥å…±äº«ã€‚å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°å…±äº«ï¼š
    
```python
import multiprocessing as mp
import logging
import time
from typing import List,Tuple,Dict,Any

# é…ç½®æ—¥å¿—
def get_logger(name=None):
    logger=logging.getLogger(name)
    if not logger.handlers:
        handler=logging.StreamHandler()
        formatter=logging.Formatter(
            '%(asctime)s - %(processName)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger
logger=get_logger(__name__)

class ShareDataProcessor:
    """æ¼”ç¤ºå¤šè¿›ç¨‹é—´æ•°æ®å…±äº«çš„å¤„ç†å™¨"""
    def __init__(self):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        logger.info("åˆå§‹åŒ–å…±äº«æ•°æ®å¤„ç†å™¨")

    # å®šä¹‰å·¥ä½œå‡½æ•°
    @staticmethod
    def value_array_worker(counter, array, worker_id):
        """Valueå’ŒArrayçš„å·¥ä½œå‡½æ•° - é™æ€æ–¹æ³•"""
        pid = mp.current_process().pid
        logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} (PIDï¼š{pid}) å¼€å§‹")

        # ä¿®æ”¹å…±äº«è®¡æ•°å™¨
        with counter.get_lock():
            counter.value += 1
            logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} å¢åŠ è®¡æ•°å™¨å€¼åˆ° {counter.value}")

        # ä¿®æ”¹å…±äº«å†…å­˜
        with array.get_lock():
            for i in range(len(array)):
                array[i] = array[i] + worker_id
            logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} æ›´æ–°æ•°ç»„ï¼š{list(array)}")
        logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} å®Œæˆ")

    # å®šä¹‰å·¥ä½œå‡½æ•°
    @staticmethod
    def manager_worker(shared_dict, shared_list, lock, worker_id):
        """Managerçš„å·¥ä½œå‡½æ•° - é™æ€æ–¹æ³•"""
        pid = mp.current_process().pid
        logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} ï¼ˆPIDï¼š{pid}ï¼‰å¼€å§‹")

        # å®‰å…¨åœ°ä¿®æ”¹å…±äº«å­—å…¸
        with lock:
            shared_dict[f'key_{worker_id}'] = worker_id * worker_id
            shared_dict['total'] = shared_dict.get('total', 0) + worker_id
            logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} æ›´æ–°å­—å…¸ï¼š{dict(shared_dict)}")

        # å®‰å…¨åœ°ä¿®æ”¹å…±äº«åˆ—è¡¨
        with lock:
            shared_list.append(f"æ¥è‡ªè¿›ç¨‹ {worker_id} çš„æ•°æ®")
            logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} æ›´æ–°åˆ—è¡¨ï¼š{list(shared_list)}")

        # æ¨¡æ‹Ÿä¸€äº›å·¥ä½œ
        time.sleep(0.5)
        logger.info(f"å·¥ä½œè¿›ç¨‹ {worker_id} å®Œæˆ")


    def using_value_and_array(self):
            """ä½¿ç”¨å…±äº«å†…å­˜Valueå’ŒArrayçš„ç¤ºä¾‹"""
            # åˆ›å»ºå…±äº«æ•´æ•°å’Œæ•°ç»„
            shared_counter=mp.Value('i',0) # 'i'è¡¨ç¤ºæœ‰ç¬¦å·æ•´æ•°
            shared_array=mp.Array('d',[0.0]*5) # 'd'è¡¨ç¤ºåŒç²¾åº¦æµ®ç‚¹æ•°
            processes=[]

            # åˆ›å»ºå¹¶å¯åŠ¨è¿›ç¨‹
            for i in range(3):
                p=mp.Process(
                    target=self.value_array_worker,
                    args=(shared_counter,shared_array,i)
                )
                processes.append(p)
                p.start()

            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            for p in processes:
                p.join()

            logger.info(f"æ‰€æœ‰è¿›ç¨‹å®Œæˆã€‚æœ€ç»ˆè®¡æ•°å™¨å€¼ï¼š{shared_counter.value}")
            logger.info(f"æ‰€æœ‰è¿›ç¨‹å®Œæˆã€‚æœ€ç»ˆæ•°ç»„å€¼ï¼š{list(shared_array)}")

    def using_manager(self):
        """ä½¿ç”¨Managerå…±äº«å¤æ‚æ•°æ®ç»“æ„ç¤ºä¾‹"""
        # åˆ›å»ºManagerå¯¹è±¡
        with mp.Manager() as manager:
            # åˆ›å»ºå…±äº«å­—å…¸å’Œåˆ—è¡¨
            shared_dict=manager.dict()
            shared_list=manager.list()

            # åˆ›å»ºå…±äº«é”
            lock=manager.RLock()
            processes=[]

            # åˆ›å»ºå¹¶è‡ªå¯åŠ¨è¿›ç¨‹
            for i in range(5):
                p=mp.Process(
                    target=self.manager_worker,
                    args=(shared_dict,shared_list,lock,i),
                    name=f"Manager-Worker-{i}"
                )
                processes.append(p)
                p.start()

            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            for p in processes:
                p.join()

            logger.info("æ‰€æœ‰è¿›ç¨‹å®Œæˆ")
            logger.info(f"æœ€ç»ˆå…±äº«å­—å…¸ï¼š{dict(shared_dict)}")
            logger.info(f"æœ€ç»ˆå…±äº«åˆ—è¡¨ï¼š{list(shared_list)}")

if __name__ == '__main__':
    processor=ShareDataProcessor()
    print("\n== ä½¿ç”¨ Value å’Œ Array å…±äº«æ•°æ® ==")
    processor.using_value_and_array()

    print("\n== ä½¿ç”¨ Manager å…±äº«æ•°æ® ==")
    processor.using_manager()
```
    
    

### è¿›ç¨‹æ± çš„æœ€ä½³å®è·µ

å¯¹äºéœ€è¦å¹¶è¡Œå¤„ç†çš„å¤§é‡ç‹¬ç«‹ä»»åŠ¡ï¼Œè¿›ç¨‹æ± æ˜¯æ›´é«˜æ•ˆçš„é€‰æ‹©ï¼š
    
```python
import logging
import multiprocessing as mp
import time
from typing import List,Tuple,Dict,Any

from concurrent.futures import ProcessPoolExecutor

import random

def get_logger(name=None):
    logger=logging.getLogger(name)
    if not logger.handlers:
        handler=logging.StreamHandler()
        formatter=logging.Formatter(
            '%(asctime)s - %(processName)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)

    return logger

# ä½¿ç”¨æ–¹å¼
logger=get_logger(__name__)

class AdvancedTaskProcessor:
    """é«˜çº§ä»»åŠ¡å¤„ç†å™¨ï¼Œæ¼”ç¤ºè¿›ç¨‹æ± æœ€ä½³å®è·µ"""
    def __init__(self,max_workers:int=None):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        å‚æ•°ï¼š
            max_workersï¼šæœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        self.max_workers=max_workers or mp.cpu_count()
        logger.info(f"åˆå§‹åŒ–é«˜çº§ä»»åŠ¡å¤„ç†å™¨ï¼Œæœ€å¤§è¿›ç¨‹æ•°ï¼š{self.max_workers}")

    def process_single_task(self,task_data:Tuple[int,int])->Tuple[int,float]:
        """
        å¤„ç†å•ä¸ªä»»åŠ¡çš„å‡½æ•°
        å‚æ•°ï¼š
            task_dataï¼šä»»åŠ¡æ•°æ®ï¼Œæ ¼å¼ä¸ºï¼ˆä»»åŠ¡IDï¼Œä»»åŠ¡å¤æ‚åº¦ï¼‰

        è¿”å›ï¼š
            Tuple[int,float]ï¼šï¼ˆä»»åŠ¡IDï¼Œå¤„ç†ç»“æœï¼‰
        """
        task_id,complexity=task_data
        process_id=mp.current_process().pid

        logger.info(f"è¿›ç¨‹ {process_id} å¼€å§‹å¤„ç†ä»»åŠ¡ {task_id}ï¼Œå¤æ‚åº¦ï¼š{complexity}")

        # æ¨¡æ‹Ÿè®¡ç®—å¯†é›†å‹å·¥ä½œ
        start_time=time.time()
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å†³å®šè®¡ç®—é‡
        iterations=complexity*1000000
        result=0
        for i in range(iterations):
            result+=i%10
            # é¿å…è¿‡åº¦ä¼˜åŒ–
            if i%1000000==0:
                result=result*0.99999

        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time=time.time()-start_time

        logger.info(f"è¿›ç¨‹ {process_id} å®Œæˆä»»åŠ¡ {task_id}ï¼Œè€—æ—¶ï¼š{processing_time:.2f} ç§’")

        return task_id,result

    def process_tasks_with_pool(self,tasks:List[Tuple[int,int]])->List[Tuple[int,float]]:
        """
        ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†å¤šä¸ªä»»åŠ¡
        å‚æ•°ï¼š
            tasksï¼šä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡ä¸ºï¼ˆä»»åŠ¡IDï¼Œä»»åŠ¡å¤æ‚åº¦ï¼‰å…ƒç»„
        è¿”å›ï¼š
            List[Tuple[int,float]]ï¼šå¤„ç†ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªè¿›åœº")

        start_time=time.time()

        results=[]

        # ä½¿ç”¨ProcessPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures=[executor.submit(self.process_single_task,task) for task in tasks]

            # æ”¶é›†ç»“æœ
            for future in futures:
                try:
                    result=future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™ï¼š{str(e)}")

        elapsed=time.time()-start_time
        logger.info(f"æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ï¼š{elapsed:.2f} ç§’")

        return results

    def run_demo(self,num_tasks:int=10,seed:int=42):
        """
        è¿è¡Œæ¼”ç¤º
        å‚æ•°ï¼š
            num_tasksï¼šä»»åŠ¡æ•°é‡
            seedï¼šéšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
        """
        random.seed(seed)

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼Œå¤æ‚åº¦åœ¨1åˆ°5ä¹‹é—´éšæœº
        tasks=[(i,random.randint(1,5)) for i in range(num_tasks)]

        logger.info(f"åˆ›å»ºäº† {num_tasks} ä¸ªå­ä»»åŠ¡")
        logger.info(f"ä»»åŠ¡åˆ—è¡¨ï¼š{tasks}")

        # å¤„ç†ä»»åŠ¡
        results=self.process_tasks_with_pool(tasks)

        # è¾“å‡ºç»“æœ
        logger.info(f"å¤„ç†ç»“æœï¼š{results}")

if __name__ == '__main__':
    processor=AdvancedTaskProcessor()
    processor.run_demo(num_tasks=8)



```
    
    

## 4\. åç¨‹ç¼–ç¨‹ï¼ˆç»­ï¼‰

### åŸºäº asyncio çš„åç¨‹ç¤ºä¾‹ï¼ˆç»­ï¼‰
    
```python
import logging
import multiprocessing as mp
import time
from typing import List,Tuple,Dict,Any

from concurrent.futures import ProcessPoolExecutor

import random

def get_logger(name=None):
    logger=logging.getLogger(name)
    if not logger.handlers:
        handler=logging.StreamHandler()
        formatter=logging.Formatter(
            '%(asctime)s - %(processName)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)

    return logger

# ä½¿ç”¨æ–¹å¼
logger=get_logger(__name__)

class AdvancedTaskProcessor:
    """é«˜çº§ä»»åŠ¡å¤„ç†å™¨ï¼Œæ¼”ç¤ºè¿›ç¨‹æ± æœ€ä½³å®è·µ"""
    def __init__(self,max_workers:int=None):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        å‚æ•°ï¼š
            max_workersï¼šæœ€å¤§å·¥ä½œè¿›ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºCPUæ ¸å¿ƒæ•°
        """
        self.max_workers=max_workers or mp.cpu_count()
        logger.info(f"åˆå§‹åŒ–é«˜çº§ä»»åŠ¡å¤„ç†å™¨ï¼Œæœ€å¤§è¿›ç¨‹æ•°ï¼š{self.max_workers}")

    def process_single_task(self,task_data:Tuple[int,int])->Tuple[int,float]:
        """
        å¤„ç†å•ä¸ªä»»åŠ¡çš„å‡½æ•°
        å‚æ•°ï¼š
            task_dataï¼šä»»åŠ¡æ•°æ®ï¼Œæ ¼å¼ä¸ºï¼ˆä»»åŠ¡IDï¼Œä»»åŠ¡å¤æ‚åº¦ï¼‰

        è¿”å›ï¼š
            Tuple[int,float]ï¼šï¼ˆä»»åŠ¡IDï¼Œå¤„ç†ç»“æœï¼‰
        """
        task_id,complexity=task_data
        process_id=mp.current_process().pid

        logger.info(f"è¿›ç¨‹ {process_id} å¼€å§‹å¤„ç†ä»»åŠ¡ {task_id}ï¼Œå¤æ‚åº¦ï¼š{complexity}")

        # æ¨¡æ‹Ÿè®¡ç®—å¯†é›†å‹å·¥ä½œ
        start_time=time.time()
        # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦å†³å®šè®¡ç®—é‡
        iterations=complexity*1000000
        result=0
        for i in range(iterations):
            result+=i%10
            # é¿å…è¿‡åº¦ä¼˜åŒ–
            if i%1000000==0:
                result=result*0.99999

        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time=time.time()-start_time

        logger.info(f"è¿›ç¨‹ {process_id} å®Œæˆä»»åŠ¡ {task_id}ï¼Œè€—æ—¶ï¼š{processing_time:.2f} ç§’")

        return task_id,result

    def process_tasks_with_pool(self,tasks:List[Tuple[int,int]])->List[Tuple[int,float]]:
        """
        ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†å¤šä¸ªä»»åŠ¡
        å‚æ•°ï¼š
            tasksï¼šä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡ä¸ºï¼ˆä»»åŠ¡IDï¼Œä»»åŠ¡å¤æ‚åº¦ï¼‰å…ƒç»„
        è¿”å›ï¼š
            List[Tuple[int,float]]ï¼šå¤„ç†ç»“æœåˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡ï¼Œä½¿ç”¨ {self.max_workers} ä¸ªè¿›åœº")

        start_time=time.time()

        results=[]

        # ä½¿ç”¨ProcessPoolExecutorè¿›è¡Œå¹¶è¡Œå¤„ç†
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures=[executor.submit(self.process_single_task,task) for task in tasks]

            # æ”¶é›†ç»“æœ
            for future in futures:
                try:
                    result=future.result()
                    results.append(result)
                except Exception as e:
                    logger.error(f"ä»»åŠ¡æ‰§è¡Œå‡ºé”™ï¼š{str(e)}")

        elapsed=time.time()-start_time
        logger.info(f"æ‰€æœ‰ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶ï¼š{elapsed:.2f} ç§’")

        return results

    def run_demo(self,num_tasks:int=10,seed:int=42):
        """
        è¿è¡Œæ¼”ç¤º
        å‚æ•°ï¼š
            num_tasksï¼šä»»åŠ¡æ•°é‡
            seedï¼šéšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
        """
        random.seed(seed)

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼Œå¤æ‚åº¦åœ¨1åˆ°5ä¹‹é—´éšæœº
        tasks=[(i,random.randint(1,5)) for i in range(num_tasks)]

        logger.info(f"åˆ›å»ºäº† {num_tasks} ä¸ªå­ä»»åŠ¡")
        logger.info(f"ä»»åŠ¡åˆ—è¡¨ï¼š{tasks}")

        # å¤„ç†ä»»åŠ¡
        results=self.process_tasks_with_pool(tasks)

        # è¾“å‡ºç»“æœ
        logger.info(f"å¤„ç†ç»“æœï¼š{results}")

if __name__ == '__main__':
    processor=AdvancedTaskProcessor()
    processor.run_demo(num_tasks=8)



```
    
    

### ä½¿ç”¨ aiohttp è¿›è¡Œé«˜æ•ˆç½‘ç»œè¯·æ±‚

åç¨‹åœ¨ç½‘ç»œ IO åœºæ™¯ä¸­ç‰¹åˆ«é«˜æ•ˆã€‚ä¸‹é¢æ˜¯ä½¿ç”¨ aiohttp å®ç°å¹¶å‘ä¸‹è½½å¤šä¸ªæ–‡ä»¶çš„ç¤ºä¾‹ï¼š
    
```python
import asyncio
import logging
import os
import time

import aiofiles
import aiohttp

from aiohttp import ClientSession
from typing import List,Tuple,Dict,Any



# é…ç½®æ—¥å¿—
def get_logger(name=None):
    logger=logging.getLogger(name)
    if not logger.handlers:
        handler=logging.StreamHandler()
        formatter=logging.Formatter(
            '%(asctime)s - %(processName)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger
logger=get_logger(__name__)

"""
å¼‚æ­¥HTTPä¸‹è½½å™¨ï¼ŒåŸºäºaiohttpå’Œasyncioå®ç°
"""
class AsyncDownloader:

    """
    åˆå§‹åŒ–ä¸‹è½½å™¨
    å‚æ•°ï¼š
        download_dirï¼šä¸‹è½½æ–‡ä»¶ä¿å­˜ç›®å½•
        chunk_sizeï¼šä¸‹è½½æ—¶çš„å—å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    """
    def __init__(self,download_dir:str="./downloads",chunk_size:int=8192):
        self.download_dir=download_dir
        self.chunk_size=chunk_size

        # ç¡®ä¿ä¸‹è½½ç›®å½•å­˜åœ¨
        os.makedirs(download_dir,exist_ok=True)
        logger.info(f"å¼‚æ­¥ä¸‹è½½å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¸‹è½½ç›®å½•ï¼š{download_dir}")

    """
    å¼‚æ­¥ä¸‹è½½å•ä¸ªæ–‡ä»¶
    å‚æ•°ï¼š
        sessionï¼šaiohttpä¼šè¯å¯¹è±¡
        urlï¼šä¸‹è½½url
        file_nameï¼šä¿å­˜çš„æ–‡ä»¶å
    è¿”å›ï¼š
        Dictï¼šä¸‹è½½ç»“æœä¿¡æ¯
    """
    async def download_file(self,session:ClientSession,url:str,filename:str)->Dict[str,Any]:
        file_path=os.path.join(self.download_dir,filename)
        start_time=time.time()

        # è®°å½•ä¸‹è½½ä¿¡æ¯
        result={
            'url':url,
            'file_name':file_name,
            'file_path':file_path,
            'success':False,
            'file_size':0,
            'download_time':0,
            'speed':0,
            'error':None
        }

        try:
            logger.info(f"å¼€å§‹ä¸‹è½½ï¼š{file_name} ä»{url}")

            # å‘é€è¯·æ±‚å¹¶æµå¼ä¸‹è½½
            async with session.get(url,ssl=False) as response:
                if response.status!=200:
                    error_msg=f"HTTPé”™è¯¯ï¼š{response.status} - {response.reason}"
                    logger.error(error_msg)
                    result['error']=error_msg

                    return result

                # è·å–æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœæœåŠ¡å™¨æä¾›ï¼‰
                content_length=response.headers.get('Content-Length')
                if content_length:
                    total_size=int(content_length)
                    result['file_size']=total_size
                    logger.info(f"{file_name} å¤§å°ï¼š{total_size/1024:.2f} KB")

                # å¼‚æ­¥å†™å…¥æ–‡ä»¶
                async with aiofiles.open(file_path,'wb') as f:
                    downloaded=0
                    last_log_time=time.time()

                    # æµå¼ä¸‹è½½ï¼Œé¿å…å†…å­˜æº¢å‡º
                    async for chunk in response.content.iter_chunked(self.chunk_size):
                        await f.write(chunk)
                        downloaded+=len(chunk)
                        # æ¯ç§’æœ€å¤šè®°å½•ä¸€æ¬¡è¿›åº¦
                        current_time=time.time()
                        if current_time-last_log_time>=1.0 and total_size>0:
                            progress=(downloaded/total_size)*100
                            logger.debug(f"{file_name} - ä¸‹è½½è¿›åº¦ï¼š{progress:.1f}%")
                            last_log_time=current_time

            # è®¡ç®—ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
            download_time=time.time()-start_time
            speed=result['file_size']/download_time if download_time>0 else 0

            result.update({
                'success':True,
                'download_time':download_time,
                'speed':speed
            })

            logger.info(f"{file_name} ä¸‹è½½å®Œæˆ - è€—æ—¶ï¼š{download_time:.2f} ç§’ - é€Ÿåº¦ï¼š{speed/1024:.1f} KB/s")

            return result

        except aiohttp.ClientError as e:
            error_msg=f"ç½‘ç»œé”™è¯¯ï¼š{str(e)}"
            logger.error(f"ä¸‹è½½ {file_name} å¤±è´¥ï¼š{error_msg}")
            result['error']=error_msg

            return result

        except asyncio.CancelledError:
            error_msg="ä¸‹è½½è¢«å–æ¶ˆ"
            logger.warning(f"{file_name} ä¸‹è½½è¢«å–æ¶ˆ")
            result['error']=error_msg

            return result

        except Exception as e:
            error_msg=f"æœªçŸ¥é”™è¯¯ï¼š{str(e)}"
            logger.error(f"ä¸‹è½½ {file_name} æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{error_msg}")
            result['error']=error_msg

            return result


    """
    æ‰¹é‡å¹¶å‘ä¸‹è½½å¤šä¸ªæ–‡ä»¶
    å‚æ•°ï¼š
        urlsï¼šåŒ…å«ï¼ˆæ–‡ä»¶åï¼ŒURLï¼‰å…ƒç»„çš„åˆ—è¡¨
        max_concurrentï¼šæœ€å¤§å¹¶å‘ä¸‹è½½æ•°
    è¿”å›ï¼š
        List[Dict]ï¼šä¸‹è½½ç»“æœåˆ—è¡¨
    """
    async def download_batch(self,urls:List[Tuple[str,str]],max_concurrent:int=5)->List[Dict[str,Any]]:
        if not urls:
            logger.warning("æ²¡æœ‰æä¾›ä¸‹è½½URL")
            return []

        logger.info(f"å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(urls)} ä¸ªæ–‡ä»¶ï¼Œæœ€å¤§å¹¶å‘æ•° {max_concurrent}")

        start_time=time.time()
        # åˆ›å»ºé™åˆ¶å¹¶å‘æ•°çš„ä¿¡å·é‡
        semaphore=asyncio.Semaphore(max_concurrent)

        # å®šä¹‰å¸¦ä¿¡å·é‡çš„ä¸‹è½½å‡½æ•°
        async def bounded_download(session,file_name,url):
            async with semaphore:
                return await self.download_file(session,url,file_name)

        # åˆ›å»ºä¼šè¯å¹¶å‘é€è¯·æ±‚
        async with aiohttp.ClientSession() as session:
            # åˆ›å»ºæ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            tasks=[
                bounded_download(session,file_name,url)
                for file_name,url in urls
            ]

            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results=await asyncio.gather(*tasks,return_exceptions=True)

        # å¤„ç†ç»“æœ
        processed_results=[]
        success_count=0

        for result in results:
            # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿå¼‚å¸¸
            if isinstance(result,Exception):
                logger.error(f"ä¸‹è½½ä»»åŠ¡å¼‚å¸¸ï¼š{str(result)}")
                processed_results.append({
                    'success':False,
                    'error':str(result)
                })
            else:
                processed_results.append(result)
                if result['success']:
                    success_count+=1

        # ç»Ÿè®¡ä¿¡æ¯
        total_time=time.time()-start_time
        success_rate=(success_count/len(urls))*100 if urls else 0
        logger.info(f"æ‰¹é‡ä¸‹è½½å®Œæˆï¼æ€»è€—æ—¶ï¼š{total_time:.2f} ç§’")
        logger.info(f"æˆåŠŸï¼š{success_count}/{len(urls)} ({success_rate:.1f}%)")

        return processed_results

"""
ä¸»å‡½æ•°
"""
async def main():
    # åˆ›å»ºä¸‹è½½å™¨å®ä¾‹
    downloader=AsyncDownloader(download_dir="./downloads")

    # å‡†å¤‡ä¸‹è½½åˆ—è¡¨ï¼ˆæ›¿æ¢ä¸ºå®é™…å¯ç”¨çš„URLï¼‰
    urls=[
        ('image1.jpg','https://example.com/image1.jpg'),
        ('image2.jpg','https://example.com/image2.jpg'),
        ('image3.jpg','https://example.com/image3.jpg'),
        ("document1.pdf","https://example.com/document1.pdf"),
        ("document2.pdf","https://example.com/document2.pdf")
    ]

    # å¼€å§‹æ‰¹é‡ä¸‹è½½
    results=await downloader.download_batch(urls,max_concurrent=3)

    # æ‰“å°ä¸‹è½½ç»“æœæ‘˜è¦
    print("\nä¸‹è½½ç»“æœæ‘˜è¦")
    for result in results:
        if result.get('success'):
            file_name=result.get('file_name')
            size_kb=result.get('file_size',0)/1024
            time_s=result.get('download_time',0)
            speed_kbps=result.get('speed',0)/1024
            print(f"âœ… {file_name}ï¼š{size_kb:.1f} KBï¼Œ{time_s:.2f} ç§’ï¼Œ{speed_kbps:.1f} KB/s")
        else:
            url=result.get('url','Unknown URL')
            error=result.get('error','Unknown error')
            print(f"âŒ {url}: {error}")

if __name__ == '__main__':
    # åœ¨Python 3.7+ä¸­å¯ä»¥ç›´æ¥ä½¿ç”¨asyncio.run()
    asyncio.run(main())
```
    
    

## 5\. å¹¶å‘æ¨¡å¼é€‰æ‹©æŒ‡å—

æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„å¹¶å‘æ¨¡å¼éå¸¸é‡è¦ï¼Œä¸‹é¢æ˜¯é€‰æ‹©æŒ‡å—ï¼š

### 5.1 å¤šçº¿ç¨‹é€‚ç”¨åœºæ™¯

  *  **IO å¯†é›†å‹ä»»åŠ¡** ï¼šæ–‡ä»¶è¯»å†™ã€ç½‘ç»œè¯·æ±‚ã€æ•°æ®åº“æ“ä½œ
  *  **éœ€è¦å…±äº«å†…å­˜** ï¼šçº¿ç¨‹é—´å¯ç›´æ¥å…±äº«å˜é‡å’Œå¯¹è±¡
  *  **ä¸å¤–éƒ¨åº“é›†æˆ** ï¼šè®¸å¤šç¬¬ä¸‰æ–¹åº“ä¸æ”¯æŒå¼‚æ­¥ï¼Œä½†æ”¯æŒå¤šçº¿ç¨‹



### 5.2 å¤šè¿›ç¨‹é€‚ç”¨åœºæ™¯

  *  **CPU å¯†é›†å‹ä»»åŠ¡** ï¼šæ•°å­¦è®¡ç®—ã€å›¾åƒå¤„ç†ã€æ•°æ®åˆ†æ
  *  **éœ€è¦éš”ç¦»å®‰å…¨** ï¼šå„è¿›ç¨‹ç‹¬ç«‹è¿è¡Œï¼Œä¸€ä¸ªè¿›ç¨‹å´©æºƒä¸å½±å“å…¶ä»–è¿›ç¨‹
  *  **å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU** ï¼šç»•è¿‡ GIL é™åˆ¶ï¼Œå®ç°çœŸæ­£çš„å¹¶è¡Œè®¡ç®—



### 5.3 åç¨‹é€‚ç”¨åœºæ™¯

  *  **é«˜å¹¶å‘ IO æ“ä½œ** ï¼šç½‘ç»œçˆ¬è™«ã€API è°ƒç”¨ã€å¾®æœåŠ¡é€šä¿¡
  *  **èµ„æºæ¶ˆè€—æ•æ„Ÿ** ï¼šåç¨‹å¼€é”€è¿œå°äºçº¿ç¨‹å’Œè¿›ç¨‹
  *  **éœ€è¦ç²¾ç»†æ§åˆ¶ä»»åŠ¡è°ƒåº¦** ï¼šåç¨‹å¯ä»¥ç²¾ç¡®æ§åˆ¶ä»»åŠ¡åˆ‡æ¢æ—¶æœº



## 6\. æ€»ç»“ä¸æœ€ä½³å®è·µ

### 6.1 ä¸»è¦æ¦‚å¿µå¯¹æ¯”

ç‰¹æ€§| å¤šçº¿ç¨‹| å¤šè¿›ç¨‹| åç¨‹  
---|---|---|---  
é€‚ç”¨ä»»åŠ¡ç±»å‹| IO å¯†é›†å‹| CPU å¯†é›†å‹| IO å¯†é›†å‹  
èµ„æºæ¶ˆè€—| ä¸­ç­‰| é«˜| ä½  
æ•°æ®å…±äº«| ç›´æ¥å…±äº«| éœ€ç‰¹æ®Šæœºåˆ¶| ç›´æ¥å…±äº«  
å¹¶è¡Œæ‰§è¡Œ| å— GIL é™åˆ¶| çœŸæ­£å¹¶è¡Œ| å•çº¿ç¨‹å†…å¹¶å‘  
å¼€å‘å¤æ‚åº¦| ä¸­ç­‰| ä¸­ç­‰| è¾ƒé«˜ï¼ˆéœ€ç†è§£å¼‚æ­¥ï¼‰  
åˆ‡æ¢å¼€é”€| ä¸­ç­‰| é«˜| æä½  
  
### 6.2 å¹¶å‘ç¼–ç¨‹æœ€ä½³å®è·µ

  1.  **æ˜ç¡®é—®é¢˜ç±»å‹** ï¼šé¦–å…ˆç¡®å®šä»»åŠ¡æ˜¯ CPU å¯†é›†å‹è¿˜æ˜¯ IO å¯†é›†å‹ï¼Œå†é€‰æ‹©å¹¶å‘æ–¹æ¡ˆã€‚
  2.  **é¿å…å…±äº«çŠ¶æ€** ï¼šå°½é‡å‡å°‘çº¿ç¨‹/è¿›ç¨‹é—´çš„æ•°æ®å…±äº«ï¼Œå¿…è¦æ—¶ä½¿ç”¨é”æˆ–æ¶ˆæ¯ä¼ é€’ã€‚
  3.  **åˆç†ä½¿ç”¨æ± åŒ–** ï¼šä½¿ç”¨çº¿ç¨‹æ± ã€è¿›ç¨‹æ± ç®¡ç†èµ„æºï¼Œé¿å…é¢‘ç¹åˆ›å»ºé”€æ¯çš„å¼€é”€ã€‚
  4.  **å–„ç”¨å¼‚æ­¥åº“** ï¼šä½¿ç”¨åç¨‹æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©æ”¯æŒå¼‚æ­¥ IO çš„åº“ï¼ˆå¦‚ aiohttpã€asyncpgï¼‰ã€‚
  5.  **å¼‚å¸¸å¤„ç†** ï¼šç¡®ä¿æ•è·å¹¶å¦¥å–„å¤„ç†æ‰€æœ‰å­ä»»åŠ¡ä¸­çš„å¼‚å¸¸ï¼Œé˜²æ­¢é™é»˜å¤±è´¥ã€‚
  6.  **è¶…æ—¶æ§åˆ¶** ï¼šè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´ï¼Œé¿å…ä»»åŠ¡æ— é™ç­‰å¾…ã€‚
  7.  **ç›‘æ§ä¸æ—¥å¿—** ï¼šå®ç°è‰¯å¥½çš„æ—¥å¿—è®°å½•ï¼Œä¾¿äºè°ƒè¯•å’Œæ€§èƒ½åˆ†æã€‚
  8.  **èµ„æºç®¡ç†** ï¼šæ­£ç¡®é‡Šæ”¾èµ„æºï¼Œä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼ˆ `with` è¯­å¥ï¼‰è‡ªåŠ¨æ¸…ç†ã€‚

  

